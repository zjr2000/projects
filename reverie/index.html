<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG" />
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG" />
  <meta property="og:url" content="URL OF THE WEBSITE" />
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200" />
  <meta property="og:image:height" content="630" />


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Reflective Instruction Tuning</title>
  <!-- <link rel="icon" type="image/x-icon" href="static/images/favicon.ico"> -->
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>

<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Reflective Instruction Tuning: Mitigating Hallucinations in Large
              Vision-Language Models</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">Jinrui Zhang<sup>1</sup>,</span>
              <span class="author-block">Teng Wang<sup>1,2</sup>,</span>
              <span class="author-block">Haigang Zhang<sup>3</sup>,</span>
              <span class="author-block">Ping Lu<sup>4</sup>,</span>
              <span class="author-block">Feng Zheng<sup>1</sup><sup>â€ </sup></span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>1</sup>Southern University of Science and Technology,</span>
              <br>
              <span class="author-block"><sup>2</sup>The University of Hong Kong,</span>
              <br>
              <span class="author-block"><sup>3</sup>Shenzhen Polytechnic University,</span>
              <br>
              <span class="author-block"><sup>4</sup>The Cloud Computing and IT Institute of ZTE Corporation</span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block">â€ Corresponding Author</span><br>
              <span class="author-block">To Appear at ECCV2024</span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- Arxiv PDF link -->
                <span class="link-block">
                  <a href="https://arxiv.org/pdf/<ARXIV PAPER ID>.pdf" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>

                <!-- Github link -->
                <span class="link-block">
                  <a href="https://github.com/YOUR REPO HERE" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- Dataset Link. -->
                <span class="link-block">
                  <a href="https://huggingface.co/datasets/zjr2000/REVERIE"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <!-- <i class="far fa-images"></i> -->
                      <p style="font-size:18px">ðŸ¤—</p>
                      <!-- ðŸ”— -->
                    </span>
                    <span>Dataset</span>
                  </a>
                </span>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>


  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <img src="static/images/reflective_instruction_tuning.png" alt="MY ALT TEXT" />
        <div class="content has-text-justified">
          <p>
            <b>Reflective Instruction Tuning.</b> Vanilla instruction tuning only trains LVLMs solely for response
            generation, lacking of supervising the learning of fine-grained reasoning details. Reflective instruction
            tuning additionally trains model to reflect the rationale underlying the response, which provide more
            fine-grained supervision (\eg, the key visual evidence and facts to reach the response, highlighted in red),
            facilitate the model learning to capture more critical information.
          </p>
        </div>
      </div>
    </div>
  </section>

  <!-- Paper abstract -->
  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              Large vision-language models (LVLMs) have shown promising performance on a variety of vision-language
              tasks. However, they remain susceptible to hallucinations, generating outputs misaligned with visual
              content or instructions. While various mitigation strategies have been proposed, they often neglect a key
              contributor to hallucinations: lack of fine-grained reasoning supervision during training. Without
              intermediate reasoning steps, models may establish superficial shortcuts between instructions and
              responses, failing to internalize the inherent reasoning logic. To address this challenge, we propose
              reflective instruction tuning, which integrates rationale learning into visual instruction tuning. Unlike
              previous methods that learning from responses only, our approach entails the model predicting rationales
              justifying why responses are correct or incorrect. This fosters a deeper engagement with the fine-grained
              reasoning underlying each response, thus enhancing the models reasoning proficiency. To facilitate this
              approach, we propose REVERIE, the first large-scale instruction-tuning dataset with ReflEctiVE RatIonalE
              annotations. REVERIE comprises 115k machine-generated reasoning instructions, each meticulously annotated
              with a corresponding pair of correct and confusing responses, alongside comprehensive rationales
              elucidating the justification behind the correctness or erroneousness of each response. Experimental
              results on multiple LVLM benchmarks reveal that reflective instruction tuning with the REVERIE dataset
              yields substantial performance gain over the baseline model demonstrating the effectiveness of reflecting
              from the rationales.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End paper abstract -->

  <!-- Dataset -->
  <section class="hero is-small">
    <div class="hero-body">
      <div class="container">
        <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <!-- Paper pipeline. -->
            <h2 class="title is-3">The REVERIE Dataset</h2>
            <div class="content has-text-justified">
              <p>
                REVERIE is the first large-scale visual instruction-tuning dataset with ReflEctiVE RatIonalE annotations. REVERIE comprises 115k machine-generated reasoning instructions, each meticulously annotated with a corresponding pair of correct and confusing responses, alongside comprehensive rationales elucidating the justification behind the correctness or erroneousness of each response. REVERIE dataset comprises 71,558 natural images. This includes 50,938 images sourced from Visual Genome, 15,706 from the COCO and 4914 images from ScienceQA. REVERIE contains 115,280 instructions paired with corresponding positive responses, and 138,897 negative responses, where each response is supplemented with a reflective rationale, rendering total 254,177 training instances. REVERIE covers four types of vision-language tasks, including multiple-choice QA, short-answer QA, open-ended QA and Yes/No questions.
              </p>
            </div>
            <img src="static/images/reverie.png" alt="MY ALT TEXT" />
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End Dataset -->


  <!-- Dataset Collection pipeline -->
  <section class="hero is-small">
    <div class="hero-body">
      <div class="container">
        <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <!-- Paper pipeline. -->
            <h2 class="title is-3">Data collection pipeline</h2>
            <div class="content has-text-justified">
              <p>
                <b>Overview of the REVERIE dataset's data collection pipeline.</b> We first employ Gemini-Vision-Pro to
                annotate the instructions, responses and rationales for each image. Gemini-Pro is then used to check
                consistency between positive and negative rationales. Inconsistent samples are filtered to maintain
                dataset quality.
              </p>
            </div>
            <img src="static/images/pipeline.png" alt="MY ALT TEXT" />
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End Model -->

  <!-- Image carousel -->
  <section class="hero is-small">
    <div class="hero-body">
      <div class="container">
          <h2 class="title is-3" style="text-align: center;">Examples from REVERIE</h2>
          <div class="content has-text-justified">
            <p>
              The rationales contain rich visual information, outside knowledge and underlying logic, providing fine-grained reasoning supervision that help address hallucinations.
            </p>
          </div>
        <div id="results-carousel" class="carousel results-carousel">
          <div class="item">
            <!-- Your image here -->
            <img src="static/images/example1.png" alt="MY ALT TEXT" />
          </div>
          <div class="item">
            <!-- Your image here -->
            <img src="static/images/example2.png" alt="MY ALT TEXT" />
          </div>
          <div class="item">
            <!-- Your image here -->
            <img src="static/images/example3.png" alt="MY ALT TEXT" />
          </div>
          <div class="item">
            <!-- Your image here -->
            <img src="static/images/example4.png" alt="MY ALT TEXT" />
          </div>
          <div class="item">
            <!-- Your image here -->
            <img src="static/images/example5.png" alt="MY ALT TEXT" />
          </div>
          <div class="item">
            <!-- Your image here -->
            <img src="static/images/example6.png" alt="MY ALT TEXT" />
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End image carousel -->


  <!--BibTex citation -->
  <!-- <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>BibTex Code Here</code></pre>
    </div>
  </section> -->
  <!--End BibTex citation -->


  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">

            <p>
              This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template"
                target="_blank">Academic Project Page Template</a> which was adopted from theÂ <a
                href="https://nerfies.github.io" target="_blank">Nerfies</a>Â project page.
              You are free to borrow the of this website, we just ask that you link back to this page in the footer.
              <br> This website is licensed under a <a rel="license"
                href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
                Commons Attribution-ShareAlike 4.0 International License</a>.
            </p>

          </div>
        </div>
      </div>
    </div>
  </footer>

  <!-- Statcounter tracking code -->

  <!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

  <!-- End of Statcounter Code -->

</body>

</html>